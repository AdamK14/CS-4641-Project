{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine data\n",
    "PATH_TO_FILE_1 = \"../raw data/aapl_2016_2020.csv\"\n",
    "PATH_TO_FILE_2 = \"../raw data/aapl_2021_2023.csv\"\n",
    "half_1 = pd.read_csv(PATH_TO_FILE_1, low_memory=False)\n",
    "half_2 = pd.read_csv(PATH_TO_FILE_2, low_memory=False)\n",
    "df = pd.concat([half_1, half_2], ignore_index=True)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Option type\n",
    "option_type = \"put\"\n",
    "\n",
    "# Convert Unix timestamps to datetime\n",
    "df['QUOTE_DATETIME'] = pd.to_datetime(df['[QUOTE_UNIXTIME]'], unit='s')\n",
    "df['EXPIRE_DATETIME'] = pd.to_datetime(df['[EXPIRE_UNIX]'], unit='s')\n",
    "\n",
    "# Load underlying price data for volatility computation\n",
    "underlying_prices = df[['QUOTE_DATETIME', '[UNDERLYING_LAST]']].drop_duplicates().set_index('QUOTE_DATETIME').sort_index()\n",
    "\n",
    "# Function to compute historical volatility\n",
    "def historical_volatility(series, window=20):\n",
    "    return np.sqrt(252) * series.pct_change().rolling(window=window).std()\n",
    "\n",
    "# Compute 20-day historical volatility\n",
    "underlying_prices['hist_vol_20d'] = historical_volatility(underlying_prices['[UNDERLYING_LAST]'])\n",
    "underlying_prices.dropna(inplace=True)\n",
    "\n",
    "# Merge historical volatility back into main dataframe\n",
    "df = df.merge(underlying_prices[['hist_vol_20d']], left_on='QUOTE_DATETIME', right_index=True, how='inner')\n",
    "\n",
    "# Columns to numeric conversion: choose columns based on option type\n",
    "if option_type == 'call':\n",
    "    numeric_cols = ['[UNDERLYING_LAST]', '[DTE]', '[STRIKE]', '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]',\n",
    "                    '[C_THETA]', '[C_RHO]', '[C_BID]', '[C_ASK]']\n",
    "else:\n",
    "    numeric_cols = ['[UNDERLYING_LAST]', '[DTE]', '[STRIKE]', '[P_DELTA]', '[P_GAMMA]', '[P_VEGA]',\n",
    "                    '[P_THETA]', '[P_RHO]', '[P_BID]', '[P_ASK]']\n",
    "\n",
    "essential_cols = ['[UNDERLYING_LAST]', '[DTE]', '[STRIKE]', 'hist_vol_20d', '[C_BID]', '[C_ASK]']\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "df.dropna(subset=essential_cols, inplace=True)\n",
    "\n",
    "# Calculate mid-price based on option type\n",
    "if option_type == 'call':\n",
    "    df['CALL_EQUI_PRICE'] = (df['[C_BID]'] + df['[C_ASK]']) / 2\n",
    "else:\n",
    "    df['PUT_EQUI_PRICE'] = (df['[P_BID]'] + df['[P_ASK]']) / 2\n",
    "\n",
    "# Drop rows with missing essential data (dynamically chosen based on option type)\n",
    "essential_cols = ['[UNDERLYING_LAST]', '[DTE]', '[STRIKE]', 'hist_vol_20d']\n",
    "if option_type == 'call':\n",
    "    essential_cols.append('CALL_EQUI_PRICE')\n",
    "else:\n",
    "    essential_cols.append('PUT_EQUI_PRICE')\n",
    "df.dropna(subset=essential_cols, inplace=True)\n",
    "\n",
    "# Prepare option data based on the option type\n",
    "if option_type == 'call':\n",
    "    option_cols = ['[UNDERLYING_LAST]', '[STRIKE]', '[DTE]', 'hist_vol_20d', \n",
    "                   '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]', '[C_THETA]', '[C_RHO]', 'CALL_EQUI_PRICE']\n",
    "    col_rename = ['underlying_price', 'strike_price', 'days_to_expiry', 'hist_volatility', \n",
    "                  'delta', 'gamma', 'vega', 'theta', 'rho', 'equilibrium_price']\n",
    "    output_file = 'processed_calls_data.csv'\n",
    "    model_file = 'call-mlp1-test.h5'\n",
    "else:\n",
    "    option_cols = ['[UNDERLYING_LAST]', '[STRIKE]', '[DTE]', 'hist_vol_20d', \n",
    "                   '[P_DELTA]', '[P_GAMMA]', '[P_VEGA]', '[P_THETA]', '[P_RHO]', 'PUT_EQUI_PRICE']\n",
    "    col_rename = ['underlying_price', 'strike_price', 'days_to_expiry', 'hist_volatility', \n",
    "                  'delta', 'gamma', 'vega', 'theta', 'rho', 'equilibrium_price']\n",
    "    output_file = 'processed_puts_data.csv'\n",
    "    model_file = 'put-mlp1-test.h5'\n",
    "\n",
    "option_df = df[option_cols].copy()\n",
    "option_df.columns = col_rename\n",
    "\n",
    "# Normalize strike price and compute time to expiry in years\n",
    "option_df['strike_price'] = option_df['strike_price'] / 1000\n",
    "option_df['time_to_expiry'] = option_df['days_to_expiry'] / 365\n",
    "option_df.drop('days_to_expiry', axis=1, inplace=True)\n",
    "\n",
    "print(option_df.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_units_per_layer = [256, 32, 1]\n",
    "layers = 3\n",
    "n_batch = 1024\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = option_df[['underlying_price', 'strike_price', 'hist_volatility', 'delta', 'gamma', 'vega', 'theta', 'rho', 'time_to_expiry']]\n",
    "y = option_df['equilibrium_price']\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create model \n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Add first layer without ativation\n",
    "model.add(Dense(n_units_per_layer[0], input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Define Keras MLP model using LeakyReLU in subsequent layers\n",
    "for i in range(1, layers - 1):\n",
    "    model.add(Dense(n_units_per_layer[i]))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "# Add last layer\n",
    "model.add(Dense(n_units_per_layer[-1]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Learning rate scheduling\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        return lr * 0.1  # reduce LR by 10x every 10 epochs\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=n_batch, validation_split=0.2, verbose=1, callbacks=[lr_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test MAE: {mae}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "eps = 1e-10  # tiny number to avoid log(0)\n",
    "plt.plot(np.log(np.array(history.history['loss']) + eps), label='Log Training Loss')\n",
    "plt.plot(np.log(np.array(history.history['val_loss']) + eps), label='Log Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss (Log MSE)')\n",
    "plt.title(f'Training and Validation Log Loss Over Epochs ({option_type} options)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_file)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Align y_test to numpy array with matching shape\n",
    "y_test_array = y_test.to_numpy().flatten()\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(y_test_array, y_pred, s=5, alpha=0.5)\n",
    "plt.xlabel('Actual Equilibrium Price')\n",
    "plt.ylabel('Predicted Equilibrium Price')\n",
    "plt.title('Predicted vs. Actual Equilibrium Prices')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_file)\n",
    "sample_entries = X.sample(10)\n",
    "sample_scaled = scaler.transform(sample_entries)\n",
    "predictions = loaded_model.predict(sample_scaled)\n",
    "\n",
    "# Compute differences and errors\n",
    "y_true = y_test.values\n",
    "diff = y_pred - y_true\n",
    "abs_error = np.abs(diff)\n",
    "percent_error = 100 * diff / y_true\n",
    "abs_percent_error = np.abs(percent_error)\n",
    "\n",
    "# Metrics\n",
    "mean_diff = np.mean(diff)\n",
    "median_diff = np.median(diff)\n",
    "std_diff = np.std(diff)\n",
    "mae = abs_error.mean()\n",
    "rmse = np.sqrt(np.mean(diff**2))\n",
    "mape = abs_percent_error.mean()\n",
    "max_diff = diff.max()\n",
    "min_diff = diff.min()\n",
    "\n",
    "# Drop NaNs for R² calculation\n",
    "valid = ~np.isnan(y_pred) & ~np.isnan(y_true)\n",
    "r2 = r2_score(y_true[valid], y_pred[valid])\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\n--- MLP Error Metrics ---\")\n",
    "print(f\"Mean Error: {mean_diff:.4f}\")\n",
    "print(f\"Median Error: {median_diff:.4f}\")\n",
    "print(f\"Standard Deviation of Error: {std_diff:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"Max Error: {max_diff:.4f}\")\n",
    "print(f\"Min Error: {min_diff:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Plot histogram of errors\n",
    "bins = np.arange(-5, 5, 0.05)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(diff, bins=bins, alpha=0.7)\n",
    "plt.xlabel(\"MLP Predicted - Actual Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"MLP1 Error Distribution ${option_type} Options\")\n",
    "plt.xlim([-5, 5])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 10 random entries\n",
    "loaded_model = load_model(model_file)\n",
    "\n",
    "# ---- Full Test Set Error Analysis ----\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred = loaded_model.predict(X_test_scaled).flatten()\n",
    "y_true = y_test.values\n",
    "\n",
    "# Compute differences and errors\n",
    "diff = y_pred - y_true\n",
    "abs_error = np.abs(diff)\n",
    "percent_error = 100 * diff / y_true\n",
    "abs_percent_error = np.abs(percent_error)\n",
    "\n",
    "# Metrics\n",
    "mean_diff = np.mean(diff)\n",
    "median_diff = np.median(diff)\n",
    "std_diff = np.std(diff)\n",
    "mae = abs_error.mean()\n",
    "rmse = np.sqrt(np.mean(diff**2))\n",
    "mape = abs_percent_error.mean()\n",
    "max_diff = diff.max()\n",
    "min_diff = diff.min()\n",
    "\n",
    "# Drop NaNs for R² calculation\n",
    "valid = ~np.isnan(y_pred) & ~np.isnan(y_true)\n",
    "r2 = r2_score(y_true[valid], y_pred[valid])\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\n--- MLP Error Metrics ---\")\n",
    "print(f\"Mean Error: {mean_diff:.4f}\")\n",
    "print(f\"Median Error: {median_diff:.4f}\")\n",
    "print(f\"Standard Deviation of Error: {std_diff:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"Max Error: {max_diff:.4f}\")\n",
    "print(f\"Min Error: {min_diff:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Create a mask to filter out zero or near-zero actual prices\n",
    "valid_mask = np.abs(y_true) > 0\n",
    "\n",
    "# Apply the mask\n",
    "y_true_valid = y_true[valid_mask]\n",
    "y_pred_valid = y_pred[valid_mask]\n",
    "\n",
    "# Now calculate percent errors without worrying about divide-by-zero\n",
    "abs_percent_errors = 100 * np.abs((y_pred_valid - y_true_valid) / y_true_valid)\n",
    "\n",
    "# PE metrics\n",
    "def compute_pe(abs_percent_errors, threshold):\n",
    "    return np.mean(abs_percent_errors <= threshold) * 100\n",
    "\n",
    "print(f\"PE5:  {compute_pe(abs_percent_errors, 5):.2f}%\")\n",
    "print(f\"PE10: {compute_pe(abs_percent_errors, 10):.2f}%\")\n",
    "print(f\"PE20: {compute_pe(abs_percent_errors, 20):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
